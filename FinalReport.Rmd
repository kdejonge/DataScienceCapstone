---
title: "Final Assignment"
author: "K de Jonge"
date: "January 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 4

The goal of this task is to build a predictive model based on the previous data modeling steps.


```{r N gram models}

library(RWeka)
library(ggplot2)

set.seed(679)
data.sample <- c(sample(blogs, length(blogs) * 0.01),
                 sample(news, length(news) * 0.01),
                 sample(twitter, length(twitter) * 0.01))

# Create corpus and clean the data
corpus <- VCorpus(VectorSource(data.sample))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)


data.frame(source = c("twitter", "news", "blogs"),
           file.size.MB = c(twittersize, blogssize, newssize),
           num.lines = c(twitterlength, newslength, blogslenght),
           num.words = c(twitterwordsz, newswordsz, blogswordsz),
           mean.num.words = (c(twitterwordsz, newswordsz, blogswordsz)/c(twitterlength, newslength, blogslenght)))


getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
makePlot <- function(data, label) {
  ggplot(data[1:30,], aes(reorder(word, -freq), freq)) +
         labs(x = label, y = "Frequency") +
         theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
         geom_bar(stat = "identity", fill = I("grey50"))
}

# Get frequencies of most common n-grams in data sample
freq1 <- getFreq(removeSparseTerms(TermDocumentMatrix(twitterdoc.corpus1), 0.9999))
freq2 <- getFreq(removeSparseTerms(TermDocumentMatrix(twitterdoc.corpus1, control = list(tokenize = bigram)), 0.9999))
freq3 <- getFreq(removeSparseTerms(TermDocumentMatrix(twitterdoc.corpus1, control = list(tokenize = trigram)), 0.9999))


clean_input <- function(x){
  work <- tolower(x)
  work <- gsub("[[:punct:]]", "", work)
  work <- gsub("+\\d+", "", work)
  work <- gsub("^ *|(?<= ) | *$", "", work, perl=T)
  return(work)
}

# 
# twitter prediction function 
#
twitter_predict <- function(x) {
  
  # load dataset
  twitter_lookup <- read.csv(file = "twitter_lookup.csv",stringsAsFactors = FALSE)
  default <- twitter_lookup[twitter_lookup$length == 1,8]
  
  # clean input
  work <- clean_input(x)
  
  #split x into n1, n2, n3, and n4 length strings
  chopped <- strsplit(work," ")
  howlong <- length(chopped[[1]])
  if(howlong < 3) { maxcount <- howlong} else { maxcount <- 3}
  
  for(i in 1:maxcount) {
    nam <- paste("n",i,"_search",sep = "")
    assign(nam,chopped[[1]][howlong+1-i])
  }
  
  #search for a 3gram
  if(maxcount > 2) {
    n3_search <- paste(n3_search,n2_search,n1_search,sep =" ")
    
    n3_full <- twitter_lookup[twitter_lookup$less1gram == n3_search,]
    if(nrow(n3_full)<1) {n3_next <- c(0,default)} else {n3_next <- n3_full[1,7:8]}
    
  } else { n3_next <- c(0,default)}
  
  #search for a 2gram
  if(maxcount > 1) {
    n2_search <- paste(n2_search,n1_search,sep =" ")
    
    n2_full <- twitter_lookup[twitter_lookup$less1gram == n2_search,]
    if(nrow(n2_full)<1) {n2_next <- c(0,default)} else {n2_next <- n2_full[1,7:8]}
    
  } else { n2_next <- c(0,default)}
  
  #search for a 1gram
  n1_full <- twitter_lookup[twitter_lookup$less1gram == n1_search,]
  if(nrow(n1_full)<1) {n1_next <- c(0,default)} else {n1_next <- n1_full[1,7:8]}
  
  if(n2_next[1] > n3_next[1]) { showdown <- n2_next} else { showdown <- n3_next }
  if(n1_next[1] > showdown[1]) { output <- n1_next} else {output <- showdown}
  
  output
}
#
# blogs prediction 
#
blogs_predict <- function(x) {
  # load dataset
  blogs_lookup <- read.csv(file = "blogs_lookup.csv",stringsAsFactors = FALSE)
  default <- blogs_lookup[blogs_lookup$length == 1,8]
  
  # clean input
  work <- clean_input(x)
  
  #split x into n1, n2, n3, and n4 length strings
  chopped <- strsplit(work," ")
  howlong <- length(chopped[[1]])
  if(howlong < 3) { maxcount <- howlong} else { maxcount <- 3}
  
  for(i in 1:maxcount) {
    nam <- paste("n",i,"_search",sep = "")
    assign(nam,chopped[[1]][howlong+1-i])
  }
  
  #search for a 3gram
  if(maxcount > 2) {
    n3_search <- paste(n3_search,n2_search,n1_search,sep =" ")
    
    n3_full <- blogs_lookup[blogs_lookup$less1gram == n3_search,]
    if(nrow(n3_full)<1) {n3_next <- c(0,default)} else {n3_next <- n3_full[1,7:8]}
    
  } else { n3_next <- c(0,default)}
  
  #search for a 2gram
  if(maxcount > 1) {
    n2_search <- paste(n2_search,n1_search,sep =" ")
    
    n2_full <- blogs_lookup[blogs_lookup$less1gram == n2_search,]
    if(nrow(n2_full)<1) {n2_next <- c(0,default)} else {n2_next <- n2_full[1,7:8]}
    
  } else { n2_next <- c(0,default)}
  
  #search for a 1gram
  n1_full <- blogs_lookup[blogs_lookup$less1gram == n1_search,]
  if(nrow(n1_full)<1) {n1_next <- c(0,default)} else {n1_next <- n1_full[1,7:8]}
  
  if(n2_next[1] > n3_next[1]) { showdown <- n2_next} else { showdown <- n3_next }
  if(n1_next[1] > showdown[1]) { output <- n1_next} else {output <- showdown}
  
  output
}

# 
# news prediction
#
news_predict <- function(x) {
  # load dataset
  news_lookup <- read.csv(file = "news_lookup.csv",stringsAsFactors = FALSE)
  default <- news_lookup[news_lookup$length == 1,8]
  
  # clean input
  work <- clean_input(x)
  
  #split x into n1, n2, n3, and n4 length strings
  chopped <- strsplit(work," ")
  howlong <- length(chopped[[1]])
  if(howlong < 3) { maxcount <- howlong} else { maxcount <- 3}
  
  for(i in 1:maxcount) {
    nam <- paste("n",i,"_search",sep = "")
    assign(nam,chopped[[1]][howlong+1-i])
  }
  
  #search for a 3gram
  if(maxcount > 2) {
    n3_search <- paste(n3_search,n2_search,n1_search,sep =" ")
    
    n3_full <- news_lookup[news_lookup$less1gram == n3_search,]
    if(nrow(n3_full)<1) {n3_next <- c(0,default)} else {n3_next <- n3_full[1,7:8]}
    
  } else { n3_next <- c(0,default)}
  
  #search for a 2gram
  if(maxcount > 1) {
    n2_search <- paste(n2_search,n1_search,sep =" ")
    
    n2_full <- news_lookup[news_lookup$less1gram == n2_search,]
    if(nrow(n2_full)<1) {n2_next <- c(0,default)} else {n2_next <- n2_full[1,7:8]}
    
  } else { n2_next <- c(0,default)}
  
  #search for a 1gram
  n1_full <- news_lookup[news_lookup$less1gram == n1_search,]
  if(nrow(n1_full)<1) {n1_next <- c(0,default)} else {n1_next <- n1_full[1,7:8]}
  
  if(n2_next[1] > n3_next[1]) { showdown <- n2_next} else { showdown <- n3_next }
  if(n1_next[1] > showdown[1]) { output <- n1_next} else {output <- showdown}
  
  output
}


```

## Task 5

The goal of the fifth task is to explore new models and data to improve your predictive model. Evaluate your new predictions on both accuracy and efficiency.


## Task 6

For the sixth task you have to Create a data product to show off your prediction algorithm You should create a Shiny app that accepts an n-gram and predicts the next word.

See UI.R en server.R

## Task 7

Create a slide deck promoting your product. Write 5 slides using RStudio Presenter explaining your product and why it is awesome!

See Rpresentation: http://rpubs.com/KdeJonge/FinalPresentation
